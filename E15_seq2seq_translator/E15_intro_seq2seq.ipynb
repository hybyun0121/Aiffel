{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. 기계 번역 이야기\n",
    "---\n",
    "![image](https://user-images.githubusercontent.com/63500940/93839119-dbd33e80-fcc6-11ea-90d7-79a14a1f8ae7.png)  \n",
    "  \n",
    "1940~1980년대까지의 기계번역은 **규칙 기반의 기계 번역(Rule-based Machine Translation)** 이 주를 이루었다.  \n",
    "  \n",
    "80년대 이후 **통계적 기계 번역(Statistical machine Translation)**  \n",
    "  \n",
    "통계 기반 번역에 여러 머신 러닝이 사용되었지만, 딥 러닝은 아니었다.  \n",
    "  \n",
    "구글은 구글 번역기에 **신경망 기게 번역(Neural Machine Translation, NMT)** 을 도입  \n",
    "  \n",
    "이때 사용된 인공 신경망이 **seq2seq** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시퀀스 처리하는 RNN\n",
    "![image](https://user-images.githubusercontent.com/63500940/93839679-adeef980-fcc8-11ea-82ad-0123419409b8.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq\n",
    "### seq2seq의 인코더-디코더 구조\n",
    "![image](https://user-images.githubusercontent.com/63500940/93839833-235aca00-fcc9-11ea-8c10-8a6cc4e385f5.png)  \n",
    "  \n",
    "아래 그림에서 Encoder는 Feature Extractor의 역할을 한다. 어떤 데이터 X를 해석하기 위한 저차원의 feature vector z를 만들어낸다. 반면 Decoder는 저차원의 feature z로부터 정보를 복원해서 다시 어떤 데이터 X'을 재생성하는 역할을 한다.  \n",
    "![image](https://user-images.githubusercontent.com/63500940/93839892-4f764b00-fcc9-11ea-8613-09d751f2d442.png)  \n",
    "  \n",
    "디코더는 인코더의 마지막 time step의 hidden state를 전달 받아 자신의 초기 hidden state로 하고, 출력 문장을 생성해내기 시작한다. 여기서는 특수 문자를 사용해서 출력 문장의 시작과 종료를 알려주어야 한다. 위의 그림에서는 _GO와 EOS가 각각 시작 문자와 종료 문자에 해당한다.  \n",
    "문헌에 따라서는 SOS와 EOS라고도 하는데, SOS는 start of sequence를 의미하고,  \n",
    "EOS는 end of sequence를 의미한다.  \n",
    "  \n",
    "## Conditional Language Model\n",
    "문장 생성기(Text Generator) 모델을 만들어 보았다면, 그러한 문장 생성기는 **언어 모델(Language Model)**을 구현한 것이라는 것을 알고 있을 것이다.  \n",
    "  \n",
    "언어 모델이란 n-1개의 단어 시퀀스 $w_1, \\dots , w_{n-1}$ 가 주어졌을 때, n 번째 단어 $w_n$ 으로 무엇이 올지를 예측하는 확률 모델이다. 파라미터 $\\theta$ 로 모델링하는 언어 모델을 다음과 같이 표현할 수 있다.  \n",
    "$$P(w_n|w_1, \\dots ,w_{n-1};\\theta)$$  \n",
    "  \n",
    "우리가 알고 있는 RNN 계열의 모델들은 이러한 언어 모델을 구현하기에 적합한 것들이다.  \n",
    "  \n",
    "그런데, 언어모델에 기반한 문장 생성기가 가지고 있는 한가지 문제점이 있다. 그것은 바로  \n",
    "[어떤 말을 만들고 싶은지를 제어할 수 없다]는 점이다.RNN 모델이 확률적으로 그 다음 나올 단어들을 순차적으로 만들어 나가게 되는데, 그것을 상황에 맞게 제어할 수 있다면 아주 유용할 것이다. 그래서 사람들은 위 언어모델을 확장해서 조건적 언어모델(Conditional Language MOdle)의 개념을 생각하게 된다. 말하자면 아래와 같다.  \n",
    "$$P(w_n|w_1, \\dots ,w_{n-1},C;\\theta)$$  \n",
    "위 식과 다르게 C라는 것이 추가되었다. c를 이용해 아무 문장이나 만들지 말고 c에 적합한 문장을 만들어라고 주문하고 싶은 것이다.  \n",
    "  \n",
    "X라는 영어문장을 Y라는 프랑스어 문장으로 번역해 => 프랑스어 문장 Y를 만들어 봐, 단 그 문장은 영어 X라는 뜻이어야한다.  \n",
    "  \n",
    "어떻게 이런 조건을 문장생성기에 넣어줄 수 있을까?  \n",
    "바로 문장 X를 해석해서 c로 만든 인코더를 또 다른 RNN으로 만드는 것이다.  \n",
    "그렇게 만든 C를 다시 문장생성기인 디코더 RNN에 입력으로 넣어주는 모델을 만들어 낸 것이 바로 오늘 다루게 될 seq2seq이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 교사 강요(teacher forcing)\n",
    "seq2seq는 훈련 과정과 테스트 과정에서의 동작 방식이 다르다는 특징이 있다.  \n",
    "테스트 과정에서는 이전 타입스텝의 출력을 현재 타임스탭의 입력으로 사용한다는 특징을 가지고 있엇지만, 학습 과정에서 만약 이전 스텝에서 생성된 단어가 잘 못된 단어라면 학습을 하기에 굉장히 오랜 시간이 필요할 것이다.  \n",
    "그래서 교사 강요방식으로 훈련을 하게된다.  \n",
    "훈련과정에서는 실제 정답을 알고 있기때문에 이전 타임 스텝의 예측값을 현재 타임스텝의 입력으로 사용하는 것이 아니라 이전 타임스텝의 실제값으로 사용할 수 있다.  \n",
    "학습의 효율은 올라가겟지만 창의적인 문장생성이 어렵다는 단점이 있다.  \n",
    "![image](https://user-images.githubusercontent.com/63500940/93841604-0b864480-fccf-11ea-866e-186c78b857d1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 수준 vs 문자 수준\n",
    "seq2seq는 **단어 수준(world level)** 또는 **문자 수준(character level)** 두 가지 방법으로 구현할 수 있다. 단어 수준이라고 함은 각 RNN의 time step의 입출력 단위가 단어 수준이라는 의미이고, 문자 수준이라 함음 RNN의 time steop의 입출력 단위가 문자 수준. 영어에서는 알파벳 단위이다.  \n",
    "  \n",
    "실제 구현 자체는 문자 수준이 좀 더 쉬운데,그 이유는 단어 수준 seq2seq의 경우 매 타임스텝에서 디코더 RNN이 출력을 결정할 때, 훈련 데이터에 있는 전체 단어 집합의 수가 30만개라면 30만개 중 하나를 선택해야 하므로 출력층의 크기가 무려 30만이어야 한다.  \n",
    "  \n",
    "하지만 문자 수준으로 구현하면, 영문자 알파벳은 26개에 불과하므로 출력층의 크기가 굉장히 작아진다.  \n",
    "여기에 대, 소문자를 구분하지 않고 특수문자를 포함한다고 하더라도 출력층의 크기는 100을 넘지 않는다.  \n",
    "  \n",
    "그렇다면 단어 수준과 문자 수준의 번역 둘 중 어느 쪽이 유리한가?  \n",
    "  \n",
    "두 방법 모두 장단점이 있다. 그리고 그 장단점은 서로 trade-off관계이기도 하다.  \n",
    "  \n",
    "'먹다'라는 단어 하나에 많은 변종이 있다. 먹다, 먹는다, 먹고, 먹지 등등. 이 의미가 모두 조금씩 다르다.  \n",
    "이렇게 따지만 단어 사전의 크기는 거의 무한대에 가까울 것이다.  \n",
    "이렇듯 단어 수준으로 접근하기엔 사전의 방대함이 걸린다. 또 한국어 일본어 같은 언어에서 띄어쓰기를 많이 신경을 써야하기 때문에 어려움이 있다.  \n",
    "  \n",
    "그럼 문자 수준으로 번역하면 어떨까? 문장 -> 단어 -> 문자로 쪼갠다는 것은 문자 수준의 글자 임베딩은 단어 안에 내재된 정보가 손실된다는 것을 의미한다. 즉, 기계가 데이터를 통해 글자가 단어를 이루어 내는 패턴까지 학습을 해내야한다. 그래서 충분한 데이터가 확보되지 않았다면 일반적으로 문자 수준의 번역이 단어 수준의 번역보다 품질이 떨어진다.  \n",
    "  \n",
    "하지만 최신 자연어처리의 흐르음은 단어 수준이나 문자 수준의 번역이 아닌, 그 사이의 subword 기반의 번역이 주를 이루고 있다.  \n",
    "\n",
    "---\n",
    "이번 시간에는 문자 수준으로 구축을 해보자 그리고 일단 문자 수준 번역기를 구축하고 나면 단어 수준 모델로 변환하는 것은 쉽다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번역기 만들기 (1) 데이터 전처리\n",
    "---\n",
    "```plaintext\n",
    "$ mkdir -p ~/aiffel/translator_seq2seq/data\n",
    "$ mkdir -p ~/aiffel/translator_seq2seq/models\n",
    "$ wget https://www.manythings.org/anki/fra-eng.zip\n",
    "$ mv fra-eng.zip  ~/aiffel/translator_seq2seq/data\n",
    "$ cd ~/aiffel/translator_seq2seq/data && unzip fra-eng.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 178009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115056</th>\n",
       "      <td>Do you really think it's no good?</td>\n",
       "      <td>Pensez-vous vraiment que ce ne soit pas bon ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101623</th>\n",
       "      <td>What do you know about pandas?</td>\n",
       "      <td>Que savez-vous des pandas ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144858</th>\n",
       "      <td>You couldn't have picked a better spot.</td>\n",
       "      <td>Vous n'auriez pas pu choisir meilleur endroit.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173583</th>\n",
       "      <td>Lincoln's biography is read by children all ar...</td>\n",
       "      <td>La biographie de Lincoln est lue par des enfan...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23470</th>\n",
       "      <td>Get rid of the gun.</td>\n",
       "      <td>Débarrassez-vous de l'arme !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "115056                  Do you really think it's no good?   \n",
       "101623                     What do you know about pandas?   \n",
       "144858            You couldn't have picked a better spot.   \n",
       "173583  Lincoln's biography is read by children all ar...   \n",
       "23470                                 Get rid of the gun.   \n",
       "\n",
       "                                                      fra  \\\n",
       "115056      Pensez-vous vraiment que ce ne soit pas bon ?   \n",
       "101623                        Que savez-vous des pandas ?   \n",
       "144858     Vous n'auriez pas pu choisir meilleur endroit.   \n",
       "173583  La biographie de Lincoln est lue par des enfan...   \n",
       "23470                        Débarrassez-vous de l'arme !   \n",
       "\n",
       "                                                       cc  \n",
       "115056  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "101623  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "144858  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "173583  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "23470   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세번째 열 제거하고, 훈련 데이터 5만개의 샘플로 줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>\\t Va ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>\\t Salut ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>\\t Salut. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>\\t Cours ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>\\t Courez ! \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eng             fra\n",
       "0   Go.      \\t Va ! \\n\n",
       "1   Hi.   \\t Salut ! \\n\n",
       "2   Hi.    \\t Salut. \\n\n",
       "3  Run!   \\t Cours ! \\n\n",
       "4  Run!  \\t Courez ! \\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용\n",
    "lines.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq 동작을 위해서 디코더의 입력과 예측에는 시작 토큰 <sos>와 종료 토큰 <eos>가 필요하다. 이번에는 각각 `\\t` 와 `\\n` 을 사용하겠다. 두 토큰을 추가하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20980</th>\n",
       "      <td>That is his house.</td>\n",
       "      <td>\\t C'est sa maison. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>I'm ready.</td>\n",
       "      <td>\\t Je suis prêt. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14688</th>\n",
       "      <td>I didn't see you.</td>\n",
       "      <td>\\t Je ne vous ai pas vue. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15983</th>\n",
       "      <td>Mail this letter.</td>\n",
       "      <td>\\t Poste cette lettre. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34512</th>\n",
       "      <td>Get that book for me.</td>\n",
       "      <td>\\t Attrape ce livre pour moi. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         eng                               fra\n",
       "20980     That is his house.            \\t C'est sa maison. \\n\n",
       "767               I'm ready.               \\t Je suis prêt. \\n\n",
       "14688      I didn't see you.      \\t Je ne vous ai pas vue. \\n\n",
       "15983      Mail this letter.         \\t Poste cette lettre. \\n\n",
       "34512  Get that book for me.  \\t Attrape ce livre pour moi. \\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 단어장(vocabulary)을 만들고, 각 단어에 부여된 고유한 정수로 텍스트 시퀀스를 정수 시퀀스로 변환하는 정수 인코딩 과정을 거친다. 이때 영어와 프랑스어 각각 단어장을 별도로 만들어 준다. 그리고 정상적으로 정수 시퀀스로 변환되었는지 3개의 행을 출력 시켜보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 3, 8], [10, 5, 8], [10, 5, 8]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 1, 19, 4, 1, 33, 1, 12],\n",
       " [11, 1, 3, 4, 13, 7, 5, 1, 33, 1, 12],\n",
       " [11, 1, 3, 4, 13, 7, 5, 14, 1, 12]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 51\n",
      "프랑스어 단어장의 크기 : 73\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어 데이터와 프랑스어 데이터의 최대 길이를 각각 구해보자. 이는 패딩(`<pad>`)을 위함이다.  \n",
    "모델에 입력될 영어, 프랑스어 시퀀스의 길이가 일정해야 하므로,  \n",
    "최대 길이로 맞추고 남는 시퀀스 뒷부분을 패딩으로 채우게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 23\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 51\n",
      "프랑스어 단어장의 크기 : 73\n",
      "영어 시퀀스의 최대 길이 23\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더의 입력으로 사용되는 영어 시퀀스와 달리, 프랑스어 시퀀스는 2가지 버전으로 나누어 준비해야 합니다. 하나는 디코더의 출력과 비교해야 할 정답 데이터로 사용해야 할 원래 목적에 따른 것입니다. 그리고 다른 하나는 이전 스텝에서 언급했던 교사 강요(Teacher forcing)을 위해 디코더의 입력으로 사용하기 위한 것입니다.  \n",
    "  \n",
    "이때, 디코더의 입력으로 사용할 시퀀스는 < eos >토큰이 필요가 없고, 디코더의 출력과 비교할 시퀀스는 < sos >가 필요가 없기 때문입니다. 가령, 영어로 'I am a person'이라는 문장을 프랑스어 'Je suis une personne'로 번역하는 번역기를 만든다고 해봅시다. 훈련 과정에서 디코더는 '< sos > Je suis une personne'를 입력받아서 'Je suis une personne < eos >'를 예측하도록 훈련되므로, 이런 방식으로 생성된 두가지 버전의 시퀀스를 준비해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 1, 19, 4, 1, 33, 1], [11, 1, 3, 4, 13, 7, 5, 1, 33, 1], [11, 1, 3, 4, 13, 7, 5, 14, 1]]\n",
      "[[1, 19, 4, 1, 33, 1, 12], [1, 3, 4, 13, 7, 5, 1, 33, 1, 12], [1, 3, 4, 13, 7, 5, 14, 1, 12]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더의 입력의 경우에는 숫자 12(< eos > 토큰)가 제거되었고, 디코더의 출력의 경우에는 숫자 11(< sos > 토큰)이 제거되었습니다. 이제 패딩을 진행합니다. 패딩을 진행하면 모든 샘플들의 길이가 정해준 길이로 동일하게 변환됩니다. 여기서는 아까 저장해두었던 가장 긴 샘플의 길이인 **`max_eng_seq_len`, `max_fra_seq_len`** 를 각각 사용한다.  \n",
    "  \n",
    "이렇게 되면 영어 데이터의 모든 샘플들은 `max_eng_seq_len`의 길이를 가지고, 프랑스어의 모든 샘플들은 `max_fra_seq_len`의 길이가 될것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 23)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 샘플들의 길이가 동일하게 변환된 것을 알 수 있다. 모든 샘플들의 길이가 동일하게 변환되는 과정에서 정해준 길이보다 짧은 데이터들은 뒤에 0(<pad>)으로 채워진다.  \n",
    "인코더의 샘플 하나를 출력해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  3  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 [19, 3, 8]이라는 3개의 단어만 있단 샘플이 뒤에 0이 채워지면서 `max_eng_seq_len`의 값인 23의 길이를 가지게 되었습니다. 이제 각 정수에 대해서 벡터화 방법으로 원-핫 인코딩을 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 23, 51)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원-핫 인코딩을 하고나서의 데이터의 크기는 (샘플의 수 × 샘플의 길이 × 단어장의 크기)가 됩니다. 원-핫 인코딩은 각 정수를 단어장의 크기를 가지는 원-핫 벡터로 인코딩하는 과정이기 때문입니다.\n",
    "\n",
    "마지막으로, 훈련과정의 validation을 위해 위에서 생성한 데이터 50000건 중 3000건만 검증데이터로 삼고, 나머지를 학습데이터로 삼겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (50000, 23, 51)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번역기 만들기 (2) 모델 훈련하기\n",
    "이번 실습은 기본적으로 케라스 창시자 프랑수아 숄레의 케라스의 seq2seq 구현 가이드 게시물인  \n",
    "<li><a href='https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html'> A ten-minute introduction to sequence-to-sequence learning</a></li> 를 참고로 하였습니다.  \n",
    "  \n",
    "우선 필요한 도구들을 임포트하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print('⏳')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 인코더 설계하기  \n",
    "  \n",
    "LSTM 셀을 설계하고 문장을 입력받으면 LSTM 셀이 마지막 time step의 hidden state와 cell state를 전달 받아 저장해준다. 앞서 인코더의 마지막 hidden state를 디코더의 첫번째 hidden state로 사용한다고 했었다.  \n",
    "일반적인 RNN의 경우에는 그것이 맞지만, 기본 RNN보다 좀 더 복잡한 LSTM의 경우 hidden state뿐 아니라, cell state라는 것이 존재한다.  \n",
    "  \n",
    "그래서 인코더의 LSTM 셀의 마지막 time step의 hidden state와 cell state를 디코더 LSTM의 첫번째 hidden state와 cell state로 전달해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "# 디코더로 전달할 hidden state, cell state를 리턴. encoder_outputs은 여기서는 불필요.\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">위의 코드를 한 줄, 한 줄 파악해볼게요.\n",
    ">\n",
    ">첫번째 줄 : 우선 LSTM의 입력 텐서를 정의해줍니다. 입력 문장을 저장하게 될 변수 텐서입니다.\n",
    ">\n",
    ">두번째 줄 : 256의 hidden_size를 가지는 LSTM 셀을 만들어줍니다. LSTM의 수용력(capacity)를 의미합니다. >return_state = True를 해서 hidden state와 cell state를 리턴받을 수 있도록 합니다.\n",
    ">\n",
    ">세번째 줄 : 입력 텐서를 입력으로 마지막 time step의 hidden state와 cell state를 결과로 받습니다.\n",
    ">\n",
    ">네번째 줄 : 마지막 time step의 hidden state와 cell state를 encoder_states라는 하나의 변수에 저장해뒀습니다. 이를 디코더에 전달하면 되겠네요.\n",
    ">\n",
    ">이제 디코더를 설계해볼까요? 디코더도 인코더랑 몇 가지 세부 사항을 제외하고 거의 똑같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세번째 줄을 보면 디코더의 인자로 `initial_state`가 추가되었는데요. _LSTM 셀의 초기 상태를 정의_ 해줄 수 있는 인자입니다. 여기서는 이전에 저장한 인코더의 `마지막 time step`의 `hidden state`와 `cell state`를 사용하였습니다. 디코더의 출력층을 설계해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매 time step마다의 다중 클래스 분류 문제이므로 프랑스어 단어장으로부터 한 가지 문자만 선택하도록 합니다. Dense의 인자로 프랑스어 단어장의 크기를 기재하고, 활성화 함수로 소프트맥스 함수를 사용. 최종적으로 인코더와 디코더를 연결해서 하나의 모델로 만들어줍니다. Model의 Input과 Output의 정의를 유심히 살펴 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "74/74 [==============================] - 5s 62ms/step - loss: 1.3534 - val_loss: 1.5184\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 0.9638 - val_loss: 1.0394\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 0.7880 - val_loss: 0.8843\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 0.6780 - val_loss: 0.7891\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 4s 53ms/step - loss: 0.6143 - val_loss: 0.7359\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 0.5747 - val_loss: 0.6839\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 0.5372 - val_loss: 0.6506\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 0.5133 - val_loss: 0.6229\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 0.4878 - val_loss: 0.6103\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 0.4656 - val_loss: 0.5747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0179fccc10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "model.fit([encoder_input_train, decoder_input_train], decoder_target_train,\n",
    "          batch_size=512,\n",
    "          epochs=10,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번역기 만들기(3) 모델 테스트 하기\n",
    "seq2seq는 훈련할 때와 테스트 단계의 동작이 다릅니다. 이를 위해서 테스트 단계의 디코더 모델은 설계를 다시 해줄 필요가 있습니다. 물론 이전에 학습된 디코더 모델의 레이어는 그대로 사용합니다. 왜 이렇게 번거로운 과정이 생기는 것일까요? Text Generator 모델을 만들어 보신 분이라면 알 수 있습니다. 훈련시에는 학습해야 할 타겟 문장을 디코더 모델의 입력, 출력 시퀀스로 넣어 주고, 디코더 모델이 타겟 문장을 한꺼번에 출력하게 할 수 있습니다. 그러나 테스트 단계에서는 그럴 수가 없습니다. 하나의 문장을 만들어 내기 위해 루프를 돌면서 단어를 하나씩 차례차례 예측해서, 예측한 단어가 다시 다음 단어를 예측할 때 사용되는 입력으로 재사용되는 과정이 진행되기 때문입니다.  \n",
    "  \n",
    "정리하면, 테스트 단계에서의 디코더의 동작 순서는 아래와 같습니다.  \n",
    "  \n",
    "1. 인코더에 입력 문장을 넣어 마지막 time step의 hidden, cell state를 얻는다.\n",
    "2. 토큰인 `\\t`를 디코더에 입력한다.\n",
    "3. 이전 time step의 출력층의 예측 결과를 현재 time step의 입력으로 한다.\n",
    "4. 3을 반족하다가 토큰인 `\\n`이 예측되면 이를 중단한다.  \n",
    "  \n",
    "이를 구현하기 위해선 time step의 출력층의 예측 결과를 현재 time step의 입력으로 사용하는 단계를 추가하기 위해서 루프를 돌며 디코더의 LSTM 셀을 마치 수동 제어하는 느낌으로 설계해야 한다.  \n",
    "코드가 좀 더 길어지게 된다.  \n",
    "  \n",
    "우선 인코더를 정의한다. encoder_inputs와 encoder_states는 이미 정의한 것들을 재사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None, 51)]        0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                [(None, 256), (None, 256) 315392    \n",
      "=================================================================\n",
      "Total params: 315,392\n",
      "Trainable params: 315,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "#이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "#이전 time step의 hidden state와 cell state를 하나의 변수에 저장 \n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "#decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "#구체적인 동작 자체는 def decode_sequence()에 구현\n",
    "decoder_outputs, state_h, state_c, = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "#현재 time_step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 time step의 출력 결과를 현재 time step의 입력으로 사용하기 위해 디코더 LSTM 셀의 동작을 좀 더 세분화해서 구현했다.  \n",
    "동작 자체는 이후에 구현할 def decode_sequence()에서 좀 더 자세히 다루겠다.  \n",
    "  \n",
    "디코더의 출력층을 재설계해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  337920      input_3[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_2[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 356,681\n",
      "Trainable params: 356,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs]+decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어에서 정수로, 정수에서 단어로 바꾸는 사전을 준비해 둡니다. 테스트 결과를 해석하기 위해선 사전이 필요할 것이다.  \n",
    "우리는 이전 스텝에서 문장을 숫자 인덱스로 바꾸는 Tokenizer를 만들면서 자동으로 만들어진 사전을 이미 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2ind = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 예측 과정을 위한 함수 `decode_sequence()`를 구현한다. `decode_sequence()`의 입력으로 들어가는 것은 번역하고자 하는 문장의 정수 시퀀스이다. `decode_sequence()` 내부에는 인코더를 구현한 encoder_model이 있어서 이 모델에 번역하고자 하는 문장의 정수 시퀀스인 `input_seq`를 입력하면, encoder_model은 마지막 시점의 hidden state를 리턴한다.  \n",
    "```plaintext\n",
    "# decode_sequence() 함수 내에 있는 코드\n",
    "states_value = encoder_model.predict(input_seq)\n",
    "```\n",
    "이 hidden state는 디코더의 첫번째 시점의 hidden state가 되고, 디코더는 이제 번역 문장을 완성하기 위한 예측 과정을 진행한다. 디코더의 예측 과정에서는 이전 시점에서 예측한 단어를 디코더의 현재 시점의 입력으로 넣어주는 작업을 진행한다. 그리고 이 작업은 종료를 의미하는 종료 토큰을 만나거나, 주어진 최대 길이를 넘을 때까지 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장:  Cours ! \n",
      "번역기가 번역한 문장:  ne sois pas de pronde ! \n",
      "-----------------------------------\n",
      "입력 문장: I left.\n",
      "정답 문장:  Je suis partie. \n",
      "번역기가 번역한 문장:  je ne suis pas de pronde. \n",
      "-----------------------------------\n",
      "입력 문장: Call us.\n",
      "정답 문장:  Appelez-nous ! \n",
      "번역기가 번역한 문장:  ne sois pas de pronde ! \n",
      "-----------------------------------\n",
      "입력 문장: How nice!\n",
      "정답 문장:  Comme c'est gentil ! \n",
      "번역기가 번역한 문장:  c'est un pronde le maine. \n",
      "-----------------------------------\n",
      "입력 문장: Turn left.\n",
      "정답 문장:  Tourne à gauche. \n",
      "번역기가 번역한 문장:  me ne suis pas de mointe. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
