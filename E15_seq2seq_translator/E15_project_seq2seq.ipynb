{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-Level로 번역기 업그레이드하기\n",
    "주어진 데이터에서 상위 33,000개의 샘플만 사용.  \n",
    "33000개 중 3000개는 테스트 데이터로 분리하여 모델을 학습한 후에 번역을 테스트하는 용도로 사용.  \n",
    "  \n",
    "### Step 1. 정제, 정규화, 전처리 (영어, 프랑스어 모두)\n",
    "---\n",
    "단어 단위 번역기를 하기 위해 고려해야할 사항들  \n",
    "  \n",
    "1. 구두점(Punctuation)을 단어와 분리하기.\n",
    "   Tokenization이라고 불리는 이 작업은 어디서부터 어디까지가 하나의 단어인지를 구분하는 작업.  \n",
    "2. 소문자로 바꾸기\n",
    "3. 띄어쓰기 단위로 토큰 수정행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 패키지 import\n",
    "#데이터 load 및 전처리\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 178009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eng       fra                                                 cc\n",
       "0   Go.      Va !  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "1   Hi.   Salut !  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "2   Hi.    Salut.  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "3  Run!   Cours !  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
       "4  Run!  Courez !  CC-BY 2.0 (France) Attribution: tatoeba.org #9..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139797</th>\n",
       "      <td>I've heard that name somewhere before.</td>\n",
       "      <td>J'ai déjà entendu ce nom quelque part.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145634</th>\n",
       "      <td>How did you happen to see them doing it?</td>\n",
       "      <td>Comment se fait-il que vous les ayez vues en t...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16201</th>\n",
       "      <td>Say it in French.</td>\n",
       "      <td>Dites-le en français.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7947</th>\n",
       "      <td>I felt cheated.</td>\n",
       "      <td>Je me suis sentie trompée.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47201</th>\n",
       "      <td>Don't be so hard on me.</td>\n",
       "      <td>Ne soyez pas si dure envers moi.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             eng  \\\n",
       "139797    I've heard that name somewhere before.   \n",
       "145634  How did you happen to see them doing it?   \n",
       "16201                          Say it in French.   \n",
       "7947                             I felt cheated.   \n",
       "47201                    Don't be so hard on me.   \n",
       "\n",
       "                                                      fra  \\\n",
       "139797             J'ai déjà entendu ce nom quelque part.   \n",
       "145634  Comment se fait-il que vous les ayez vues en t...   \n",
       "16201                               Dites-le en français.   \n",
       "7947                           Je me suis sentie trompée.   \n",
       "47201                    Ne soyez pas si dure envers moi.   \n",
       "\n",
       "                                                       cc  \n",
       "139797  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "145634  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "16201   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "7947    CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "47201   CC-BY 2.0 (France) Attribution: tatoeba.org #6...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>I remembered it.</td>\n",
       "      <td>Je m'en souvenais.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4465</th>\n",
       "      <td>They're dead.</td>\n",
       "      <td>Elles sont décédées.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31528</th>\n",
       "      <td>She turned him down.</td>\n",
       "      <td>Elle l'a éconduit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29250</th>\n",
       "      <td>His novel sold well.</td>\n",
       "      <td>Son roman se vendit bien.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28450</th>\n",
       "      <td>Did I tell you that?</td>\n",
       "      <td>Vous ai-je conté cela ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng                        fra\n",
       "11310      I remembered it.         Je m'en souvenais.\n",
       "4465          They're dead.       Elles sont décédées.\n",
       "31528  She turned him down.         Elle l'a éconduit.\n",
       "29250  His novel sold well.  Son roman se vendit bien.\n",
       "28450  Did I tell you that?    Vous ai-je conté cela ?"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습에 사용할 상위 33000개의 데이터만 뽑아 온다.\n",
    "lines = lines[['eng', 'fra']][:33000]\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    sent = sent.lower()\n",
    "    #단어와 구두점 사이 공백\n",
    "    sent = re.sub(r\"([?.!])\",r\" \\1 \", sent)\n",
    "    \n",
    "    #남길 단어 이외 단어들은 전부 공백(한 칸 띄어쓰기)\n",
    "    sent = re.sub(r\"[^a-zA-Z?.!]+\", r\" \", sent)\n",
    "    \n",
    "    #공백 관리하기\n",
    "    sent = re.sub(r'\\s+', \" \", sent)\n",
    "    sent = sent.strip()\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have you had dinner ?\n",
      "b'avez vous d j din ?'\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "print(preprocess_sentence(en_sent))\n",
    "print(preprocess_sentence(fr_sent).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = [preprocess_sentence(x).split() for x in lines.eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_sent = [preprocess_sentence(x) for x in lines.fra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'va', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [w for w in (\"<sos> \" + fra_sent[0]).split()]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. 디코더의 문장에서 시작 토큰과 종료 토큰 넣기\n",
    "---\n",
    "디코더의 입력 시퀀스 맨 앞에는 시작을 의미하는 토큰이 필요하다. 그리고 교사 강요를 수행할 때, 디코더의 실제값이 되는 디코더의 레이블 시퀀스에는 종료를 의미하는 종료 토큰이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teacher_forcing(sents):\n",
    "    '''\n",
    "    target sentence에 대하여\n",
    "    decoder_input => decoder input : <sos> + sentence 와\n",
    "    decoder_target => target line : sentence + <eos>를 생성\n",
    "    '''\n",
    "    decoder_input, decoder_target = [], []\n",
    "    \n",
    "    for sent in sents:\n",
    "        tar_line_input = [w for w in (\"<sos> \" + sent).split()]\n",
    "        decoder_input.append(tar_line_input)\n",
    "        \n",
    "        tar_line_target = [w for w in (sent + \" <eos>\").split()]\n",
    "        decoder_target.append(tar_line_target)\n",
    "        \n",
    "    return decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input, decoder_target = teacher_forcing(fra_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', '.'], ['hi', '.'], ['hi', '.'], ['run', '!'], ['run', '!']]\n",
      "[['<sos>', 'va', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.'], ['<sos>', 'cours', '!'], ['<sos>', 'courez', '!']]\n",
      "[['va', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>'], ['cours', '!', '<eos>'], ['courez', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[:5])\n",
    "print(decoder_input[:5])\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 케라스의 토크나이저로 텍스트를 숫자로 바꾸자.\n",
    "---\n",
    "참고 : https://wikidocs.net/31766  \n",
    "  \n",
    "위 링크의 가이드를 통해서 영어와 프랑스어에 대한 토크나이저를 각각 생성하고,  \n",
    "tokenizer.texts_to_sequences()를 사용하여 모든 샘플에 대해서 정수 시퀀스로 변환해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(encoder_input,decoder_input,decoder_target, _type='encoder'):\n",
    "    tokenizer = Tokenizer(filters=\"\",\n",
    "                         lower=False, oov_token=\"<unk>\")\n",
    "    \n",
    "    if _type == 'encoder':\n",
    "        tokenizer.fit_on_texts(encoder_input)\n",
    "        tensor = tokenizer.texts_to_sequences(encoder_input)\n",
    "        tensor = pad_sequences(tensor, padding='post')\n",
    "        return tensor, tokenizer\n",
    "    \n",
    "    elif _type == 'decoder':\n",
    "        tokenizer.fit_on_texts(decoder_input)\n",
    "        tokenizer.fit_on_texts(decoder_target)\n",
    "        dec_tensor = tokenizer.texts_to_sequences(decoder_input)\n",
    "        tar_tensor = tokenizer.texts_to_sequences(decoder_target)\n",
    "        dec_tensor = pad_sequences(dec_tensor, padding='post')\n",
    "        tar_tensor = pad_sequences(tar_tensor, padding='post')\n",
    "        return dec_tensor, tar_tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2vector, tokenizer_eng = tokenize(encoder_input,decoder_input, decoder_target,_type='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec2vector, tar2vector, tokenizer_fra = tokenize(encoder_input,decoder_input, decoder_target,_type='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33000, 8)\n",
      "(33000, 16)\n",
      "(33000, 16)\n"
     ]
    }
   ],
   "source": [
    "#토큰화된 eng, fra_dec, fra_target shape 확인하기\n",
    "print(np.shape(eng2vector))\n",
    "print(np.shape(dec2vector))\n",
    "print(np.shape(tar2vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">샘플은 총 33,000개 존재하며 영어 문장의 길이는 8, 프랑스어 문장의 길이는 16이다. 단어 집합의 크기를 정의한다.  \n",
    "<a herf=\"https://wikidocs.net/86900\">2) Word-Level 번역기 만들기(Neural Machine Translation (seq2seq) Tutorial)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 4663, 프랑스어 단어 집합의 크기 : 7327\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_eng.word_index) +1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) +1\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary_1 : key(word) : value(num)\n",
    "#dictionary_2 : key(num) : value(word)\n",
    "#딕셔너리 타입으로 단어와 토큰, 토큰과 단어가 연결되어 있는 사전을 미리 받아두자\n",
    "\n",
    "src_to_index = tokenizer_eng.word_index\n",
    "index_to_src = tokenizer_eng.index_word # 훈련 후 결과 비교할 때 사용\n",
    "\n",
    "tar_to_index = tokenizer_fra.word_index # 훈련 후 예측 과정에서 사용\n",
    "index_to_tar = tokenizer_fra.index_word # 훈련 후 결과 비교할 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">테스트 데이터를 분리할 차례이다. 테스트 데이터를 분리하기 전에, 적절한 분포를 갖도록 데이터를 섞어주는 과정을 진행한다. 이를 위해서 우선 순서가 섞인 정수 시퀀스 리스트를 만든다.  \n",
    "  \n",
    "왜냐하면 우리는 데이터셋 상위 33,000개의 데이터를 가져왔기 때문에 데이터셋 순서대로 학습을 시키는 것은 모델이 loss를 minimize하는 것을 힘들게 한다. 힘든건 나로 충분하니까 데이터를 잘 섞어주자.  \n",
    "  \n",
    "<a href=https://datascience.stackexchange.com/questions/24511/why-should-the-data-be-shuffled-for-machine-learning-tasks>Why should the data be shuffled for machine learning tasks</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10191 10713 31923 ... 26475 14406 18229]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(eng2vector.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2vector = eng2vector[indices]\n",
    "dec2vector = dec2vector[indices]\n",
    "tar2vector = tar2vector[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23  4 97 42  5  0  0  0]\n",
      "[   3    6   29   34    8 1277   65    7    0    0    0    0    0    0\n",
      "    0    0]\n",
      "[   6   29   34    8 1277   65    7    4    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "#셔플링이 잘 이루어졌는지 확인해보자\n",
    "print(eng2vector[28256])\n",
    "print(dec2vector[28256])\n",
    "print(tar2vector[28256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300\n"
     ]
    }
   ],
   "source": [
    "#데이터셋 10%를 테스트 데이터로 분리하기\n",
    "num_of_testset = int(33000*0.1)\n",
    "print(num_of_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2vector_train = eng2vector[:-num_of_testset]\n",
    "dec2vector_train = dec2vector[:-num_of_testset]\n",
    "tar2vector_train = tar2vector[:-num_of_testset]\n",
    "\n",
    "eng2vector_test = eng2vector[-num_of_testset:]\n",
    "dec2vector_test = dec2vector[-num_of_testset:]\n",
    "tar2vector_test = tar2vector[-num_of_testset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29700, 8)\n",
      "(29700, 16)\n",
      "(29700, 16)\n",
      "(3300, 8)\n",
      "(3300, 16)\n",
      "(3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print(eng2vector_train.shape)\n",
    "print(dec2vector_train.shape)\n",
    "print(tar2vector_train.shape)\n",
    "print(eng2vector_test.shape)\n",
    "print(dec2vector_test.shape)\n",
    "print(tar2vector_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. 임베딩 층 (Embedding layer)사용하기\n",
    "---\n",
    "1. 케라스 임베딩 층(keras Embedding layer) 참고하기\n",
    "https://wikidocs.net/33793  \n",
    "  \n",
    "```python\n",
    "from tensorflow.keras.layers import Input, Embedding, Masking\n",
    "\n",
    "# 인코더에서 사용할 임베딩 층 사용 예시\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(단어장의 크기, 임베딩 벡터의 차원)(encoder_inputs)\n",
    "encoder_lstm = LSTM(hidden state의 크기, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "```  \n",
    "  \n",
    "주의할 점은 인코더와 디코더의 임베딩 층은 서로 다른 임베딩 층을 사용해야하지만,  \n",
    "디코더의 훈련 과정과 테스트 과정(예측 과정)에서의 임베딩 층은 동일해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from CosineAnnealing import CosineAnnealingScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 250\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model checkpoint\n",
    "import os\n",
    "\n",
    "chekpoint_filepath = os.getenv('HOME') + '/project/E15/tmp/chekpoint'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "                            filepath = chekpoint_filepath,\n",
    "                            monitor='val_acc',\n",
    "                            save_weights_only=True,\n",
    "                            mode='max',\n",
    "                            save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping_callback = EarlyStopping(monitor='val_acc',\n",
    "                                      patience=10,\n",
    "                                      mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_callback = CosineAnnealingScheduler(T_max=epochs, eta_max=0.05, eta_min=1e-2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = [model_checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(src_vocab_size, latent_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, latent_dim) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 모델 구현하기\n",
    "---\n",
    "글자 단위 번역기에서 구현한 모델을 참고로 단어 단위 번역기의 모델을 완성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_25\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, None, 250)    1165750     input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_21 (Embedding)        (None, None, 250)    1831750     input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masking_20 (Masking)            (None, None, 250)    0           embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "masking_21 (Masking)            (None, None, 250)    0           embedding_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  [(None, 250), (None, 501000      masking_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  [(None, None, 250),  501000      masking_21[0][0]                 \n",
      "                                                                 lstm_20[0][1]                    \n",
      "                                                                 lstm_20[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, None, 7327)   1839077     lstm_21[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,838,577\n",
      "Trainable params: 5,838,577\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "233/233 [==============================] - 11s 48ms/step - loss: 2.0404 - acc: 0.6849 - val_loss: 1.6444 - val_acc: 0.7348\n",
      "Epoch 2/100\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.4782 - acc: 0.7567 - val_loss: 1.3709 - val_acc: 0.7777\n",
      "Epoch 3/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 1.2571 - acc: 0.7901 - val_loss: 1.2288 - val_acc: 0.7953\n",
      "Epoch 4/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 1.1239 - acc: 0.8087 - val_loss: 1.1268 - val_acc: 0.8098\n",
      "Epoch 5/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 1.0237 - acc: 0.8214 - val_loss: 1.0616 - val_acc: 0.8202\n",
      "Epoch 6/100\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.9433 - acc: 0.8319 - val_loss: 1.0274 - val_acc: 0.8220\n",
      "Epoch 7/100\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.8746 - acc: 0.8411 - val_loss: 0.9505 - val_acc: 0.8354\n",
      "Epoch 8/100\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.8156 - acc: 0.8490 - val_loss: 0.9153 - val_acc: 0.8409\n",
      "Epoch 9/100\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.7649 - acc: 0.8565 - val_loss: 0.9095 - val_acc: 0.8375\n",
      "Epoch 10/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.7186 - acc: 0.8637 - val_loss: 0.8788 - val_acc: 0.8457\n",
      "Epoch 11/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.6773 - acc: 0.8702 - val_loss: 0.8315 - val_acc: 0.8523\n",
      "Epoch 12/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.6406 - acc: 0.8762 - val_loss: 0.8134 - val_acc: 0.8558\n",
      "Epoch 13/100\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 0.6064 - acc: 0.8827 - val_loss: 0.8004 - val_acc: 0.8576\n",
      "Epoch 14/100\n",
      "233/233 [==============================] - 11s 48ms/step - loss: 0.5755 - acc: 0.8885 - val_loss: 0.7958 - val_acc: 0.8578\n",
      "Epoch 15/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.5469 - acc: 0.8936 - val_loss: 0.7714 - val_acc: 0.8627\n",
      "Epoch 16/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.5197 - acc: 0.8986 - val_loss: 0.7559 - val_acc: 0.8651\n",
      "Epoch 17/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.4947 - acc: 0.9033 - val_loss: 0.7553 - val_acc: 0.8649\n",
      "Epoch 18/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.4718 - acc: 0.9076 - val_loss: 0.7466 - val_acc: 0.8666\n",
      "Epoch 19/100\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 0.4503 - acc: 0.9115 - val_loss: 0.7407 - val_acc: 0.8678\n",
      "Epoch 20/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.4308 - acc: 0.9157 - val_loss: 0.7264 - val_acc: 0.8704\n",
      "Epoch 21/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.4126 - acc: 0.9190 - val_loss: 0.7547 - val_acc: 0.8664\n",
      "Epoch 22/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.3963 - acc: 0.9225 - val_loss: 0.7297 - val_acc: 0.8697\n",
      "Epoch 23/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.3807 - acc: 0.9253 - val_loss: 0.7321 - val_acc: 0.8707\n",
      "Epoch 24/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.3677 - acc: 0.9281 - val_loss: 0.7207 - val_acc: 0.8732\n",
      "Epoch 25/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.3540 - acc: 0.9305 - val_loss: 0.7176 - val_acc: 0.8735\n",
      "Epoch 26/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.3414 - acc: 0.9330 - val_loss: 0.7196 - val_acc: 0.8746\n",
      "Epoch 27/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.3300 - acc: 0.9352 - val_loss: 0.7220 - val_acc: 0.8743\n",
      "Epoch 28/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.3199 - acc: 0.9369 - val_loss: 0.7231 - val_acc: 0.8733\n",
      "Epoch 29/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.3116 - acc: 0.9388 - val_loss: 0.7193 - val_acc: 0.8747\n",
      "Epoch 30/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.3024 - acc: 0.9405 - val_loss: 0.7207 - val_acc: 0.8750\n",
      "Epoch 31/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.2945 - acc: 0.9420 - val_loss: 0.7217 - val_acc: 0.8746\n",
      "Epoch 32/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.2866 - acc: 0.9434 - val_loss: 0.7186 - val_acc: 0.8759\n",
      "Epoch 33/100\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 0.2797 - acc: 0.9444 - val_loss: 0.7291 - val_acc: 0.8740\n",
      "Epoch 34/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.2729 - acc: 0.9455 - val_loss: 0.7210 - val_acc: 0.8753\n",
      "Epoch 35/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.2665 - acc: 0.9469 - val_loss: 0.7257 - val_acc: 0.8761\n",
      "Epoch 36/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.2610 - acc: 0.9477 - val_loss: 0.7215 - val_acc: 0.8752\n",
      "Epoch 37/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.2552 - acc: 0.9489 - val_loss: 0.7279 - val_acc: 0.8757\n",
      "Epoch 38/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.2497 - acc: 0.9495 - val_loss: 0.7273 - val_acc: 0.8744\n",
      "Epoch 39/100\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 0.2448 - acc: 0.9504 - val_loss: 0.7241 - val_acc: 0.8755\n",
      "Epoch 40/100\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 0.2402 - acc: 0.9512 - val_loss: 0.7339 - val_acc: 0.8742\n",
      "Epoch 41/100\n",
      "233/233 [==============================] - 11s 45ms/step - loss: 0.2361 - acc: 0.9519 - val_loss: 0.7245 - val_acc: 0.8767\n",
      "Epoch 42/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.2326 - acc: 0.9522 - val_loss: 0.7307 - val_acc: 0.8743\n",
      "Epoch 43/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.2287 - acc: 0.9530 - val_loss: 0.7339 - val_acc: 0.8750\n",
      "Epoch 44/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.2258 - acc: 0.9533 - val_loss: 0.7331 - val_acc: 0.8750\n",
      "Epoch 45/100\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.2230 - acc: 0.9533 - val_loss: 0.7315 - val_acc: 0.8767\n",
      "Epoch 46/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.2197 - acc: 0.9539 - val_loss: 0.7342 - val_acc: 0.8745\n",
      "Epoch 47/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.2165 - acc: 0.9543 - val_loss: 0.7325 - val_acc: 0.8752\n",
      "Epoch 48/100\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.2133 - acc: 0.9548 - val_loss: 0.7363 - val_acc: 0.8750\n",
      "Epoch 49/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.2101 - acc: 0.9554 - val_loss: 0.7397 - val_acc: 0.8747\n",
      "Epoch 50/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.2072 - acc: 0.9557 - val_loss: 0.7385 - val_acc: 0.8753\n",
      "Epoch 51/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.2046 - acc: 0.9560 - val_loss: 0.7415 - val_acc: 0.8735\n",
      "Epoch 52/100\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.2026 - acc: 0.9562 - val_loss: 0.7448 - val_acc: 0.8733\n",
      "Epoch 53/100\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.2006 - acc: 0.9566 - val_loss: 0.7423 - val_acc: 0.8750\n",
      "Epoch 54/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1984 - acc: 0.9568 - val_loss: 0.7514 - val_acc: 0.8732\n",
      "Epoch 55/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1965 - acc: 0.9574 - val_loss: 0.7516 - val_acc: 0.8742\n",
      "Epoch 56/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1950 - acc: 0.9572 - val_loss: 0.7462 - val_acc: 0.8752\n",
      "Epoch 57/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1932 - acc: 0.9572 - val_loss: 0.7484 - val_acc: 0.8740\n",
      "Epoch 58/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1913 - acc: 0.9577 - val_loss: 0.7479 - val_acc: 0.8743\n",
      "Epoch 59/100\n",
      "233/233 [==============================] - 11s 45ms/step - loss: 0.1901 - acc: 0.9577 - val_loss: 0.7454 - val_acc: 0.8758\n",
      "Epoch 60/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1884 - acc: 0.9576 - val_loss: 0.7508 - val_acc: 0.8738\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 11s 45ms/step - loss: 0.1869 - acc: 0.9578 - val_loss: 0.7509 - val_acc: 0.8742\n",
      "Epoch 62/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1851 - acc: 0.9583 - val_loss: 0.7510 - val_acc: 0.8743\n",
      "Epoch 63/100\n",
      "233/233 [==============================] - 11s 45ms/step - loss: 0.1834 - acc: 0.9581 - val_loss: 0.7513 - val_acc: 0.8751\n",
      "Epoch 64/100\n",
      "233/233 [==============================] - 11s 45ms/step - loss: 0.1820 - acc: 0.9585 - val_loss: 0.7503 - val_acc: 0.8742\n",
      "Epoch 65/100\n",
      "233/233 [==============================] - 11s 45ms/step - loss: 0.1808 - acc: 0.9583 - val_loss: 0.7507 - val_acc: 0.8737\n",
      "Epoch 66/100\n",
      "233/233 [==============================] - 11s 45ms/step - loss: 0.1794 - acc: 0.9585 - val_loss: 0.7539 - val_acc: 0.8742\n",
      "Epoch 67/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1779 - acc: 0.9587 - val_loss: 0.7533 - val_acc: 0.8748\n",
      "Epoch 68/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1767 - acc: 0.9590 - val_loss: 0.7517 - val_acc: 0.8749\n",
      "Epoch 69/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1751 - acc: 0.9591 - val_loss: 0.7578 - val_acc: 0.8729\n",
      "Epoch 70/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1737 - acc: 0.9592 - val_loss: 0.7553 - val_acc: 0.8736\n",
      "Epoch 71/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1725 - acc: 0.9595 - val_loss: 0.7537 - val_acc: 0.8740\n",
      "Epoch 72/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1702 - acc: 0.9595 - val_loss: 0.7585 - val_acc: 0.8736\n",
      "Epoch 73/100\n",
      "233/233 [==============================] - 11s 45ms/step - loss: 0.1687 - acc: 0.9597 - val_loss: 0.7546 - val_acc: 0.8752\n",
      "Epoch 74/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1669 - acc: 0.9598 - val_loss: 0.7540 - val_acc: 0.8748\n",
      "Epoch 75/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1654 - acc: 0.9601 - val_loss: 0.7552 - val_acc: 0.8749\n",
      "Epoch 76/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1643 - acc: 0.9602 - val_loss: 0.7580 - val_acc: 0.8745\n",
      "Epoch 77/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1638 - acc: 0.9602 - val_loss: 0.7586 - val_acc: 0.8733\n",
      "Epoch 78/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1628 - acc: 0.9603 - val_loss: 0.7568 - val_acc: 0.8746\n",
      "Epoch 79/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1623 - acc: 0.9601 - val_loss: 0.7631 - val_acc: 0.8736\n",
      "Epoch 80/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1611 - acc: 0.9607 - val_loss: 0.7635 - val_acc: 0.8735\n",
      "Epoch 81/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1601 - acc: 0.9607 - val_loss: 0.7626 - val_acc: 0.8739\n",
      "Epoch 82/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1595 - acc: 0.9607 - val_loss: 0.7664 - val_acc: 0.8733\n",
      "Epoch 83/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1586 - acc: 0.9607 - val_loss: 0.7631 - val_acc: 0.8735\n",
      "Epoch 84/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1579 - acc: 0.9607 - val_loss: 0.7645 - val_acc: 0.8724\n",
      "Epoch 85/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1566 - acc: 0.9607 - val_loss: 0.7635 - val_acc: 0.8724\n",
      "Epoch 86/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1557 - acc: 0.9609 - val_loss: 0.7694 - val_acc: 0.8720\n",
      "Epoch 87/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1545 - acc: 0.9609 - val_loss: 0.7688 - val_acc: 0.8726\n",
      "Epoch 88/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1540 - acc: 0.9610 - val_loss: 0.7662 - val_acc: 0.8746\n",
      "Epoch 89/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1529 - acc: 0.9610 - val_loss: 0.7650 - val_acc: 0.8746\n",
      "Epoch 90/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1523 - acc: 0.9611 - val_loss: 0.7674 - val_acc: 0.8725\n",
      "Epoch 91/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1517 - acc: 0.9612 - val_loss: 0.7645 - val_acc: 0.8733\n",
      "Epoch 92/100\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.1509 - acc: 0.9614 - val_loss: 0.7635 - val_acc: 0.8734\n",
      "Epoch 93/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1503 - acc: 0.9613 - val_loss: 0.7644 - val_acc: 0.8731\n",
      "Epoch 94/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1494 - acc: 0.9613 - val_loss: 0.7642 - val_acc: 0.8728\n",
      "Epoch 95/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1486 - acc: 0.9614 - val_loss: 0.7631 - val_acc: 0.8737\n",
      "Epoch 96/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1478 - acc: 0.9614 - val_loss: 0.7654 - val_acc: 0.8730\n",
      "Epoch 97/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1470 - acc: 0.9615 - val_loss: 0.7639 - val_acc: 0.8732\n",
      "Epoch 98/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1461 - acc: 0.9616 - val_loss: 0.7668 - val_acc: 0.8723\n",
      "Epoch 99/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1454 - acc: 0.9617 - val_loss: 0.7681 - val_acc: 0.8718\n",
      "Epoch 100/100\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 0.1445 - acc: 0.9620 - val_loss: 0.7718 - val_acc: 0.8719\n"
     ]
    }
   ],
   "source": [
    "history =model.fit([eng2vector_train, dec2vector_train], tar2vector_train,\n",
    "             batch_size=128,\n",
    "             epochs=epochs,\n",
    "             callbacks=callback,\n",
    "             validation_data=([eng2vector_test, dec2vector_test], tar2vector_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619507789611816\n",
      "0.871874988079071\n",
      "0.14448168873786926\n",
      "0.7717998623847961\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'r', label='val acc')\n",
    "\n",
    "loss_ax.plot(history.history['loss'],'g', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'],'y', label='val loss')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(history.history['acc'][-1])\n",
    "print(history.history['val_acc'][-1])\n",
    "print(history.history['loss'][-1])\n",
    "print(history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6d2890c050>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model\n",
    "best_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "best_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
    "             metrics=['acc'])\n",
    "best_model.load_weights(chekpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7f6c1a9edfd0>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 모델 평가하기\n",
    "---\n",
    "단어 단위 번역기에 대해서 훈련 데이터의 샘플과 테스트 데이터의 샘플에 대해서 번역 문장을 만들어보고 정답 문장과 번역 문장을 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_20 (Embedding)     (None, None, 250)         1165750   \n",
      "_________________________________________________________________\n",
      "masking_20 (Masking)         (None, None, 250)         0         \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               [(None, 250), (None, 250) 501000    \n",
      "=================================================================\n",
      "Total params: 1,666,750\n",
      "Trainable params: 1,666,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 정수 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "         # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + index_to_src[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=tar_to_index['<sos>']) and i!=tar_to_index['<eos>']):\n",
    "            temp = temp + index_to_tar[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  they spoke briefly . \n",
      "번역문 : ils se sont bri vement entretenus . \n",
      "예측문 :  elles parl rent bri vement . \n",
      "\n",
      "\n",
      "원문 :  i got really mad . \n",
      "번역문 : je me suis vraiment mise en col re . \n",
      "예측문 :  je me suis vraiment mis en col re . \n",
      "\n",
      "\n",
      "원문 :  i wonder who . \n",
      "번역문 : je me demande qui . \n",
      "예측문 :  je me demande qui . \n",
      "\n",
      "\n",
      "원문 :  it was very ugly . \n",
      "번역문 : c tait tr s laid . \n",
      "예측문 :  c tait fort laid . \n",
      "\n",
      "\n",
      "원문 :  she became pregnant . \n",
      "번역문 : elle est tomb e enceinte . \n",
      "예측문 :  elle est tomb e enceinte . \n",
      "\n",
      "\n",
      "원문 :  i ve just seen tom . \n",
      "번역문 : je viens de voir tom . \n",
      "예측문 :  je viens de voir tom . \n",
      "\n",
      "\n",
      "원문 :  she met her uncle . \n",
      "번역문 : elle rencontra son oncle . \n",
      "예측문 :  elle a rencontr son oncle . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001, 162, 369]:\n",
    "    input_seq = eng2vector_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    print(\"원문 : \",seq2src(eng2vector_train[seq_index]))\n",
    "    print(\"번역문 :\",seq2tar(dec2vector_train[seq_index]))\n",
    "    print(\"예측문 :\",decoded_sentence[:-5])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 고찰\n",
    "  \n",
    "인코딩과 디코딩 두개의 모델을 나눠서 학습을 시키는 과정이 생소했기 때문에 모델을 구현하는데에 어려움이 있었다.  \n",
    "자연어 처리에서 항상 어려움이 있었던 토크나이즈와 임베딩 개념을 확실히 할 수 있는 시간이었다.  \n",
    "  \n",
    "Callback함수를 이용하여 모델 학습에 효율성을 높일려는 시도를 하였다.  \n",
    "CosineAnnealingScheduler 클래스를 선언하여 학습률 조절을 시도하였는데 Validation data가 수렵해나가는 속도보다 학습률이 더 빠르게 떨어져 동일한 에폭으로 실험을 했을 때 고정 learning rate로 학습을 하는 것이 오히려 더 좋은 정확도와 더 낮은 loss를 얻을 수 있었다. 시도는 좋았으나 경험적으로 아직 부족하다고 판단된다.  \n",
    "  \n",
    "이미지는 동일한 사이즈를 가진다는 데이터를 제한하는 조건이 몇 개의 샘플만 시각화 하여도 데이터 전처리가 잘 적용되고 있는지 확인이 가능하지만 자연어는 가지고 있는 데이터의 길이가 다양하기 때문에 직관적으로 전처리가 잘 되고 있는지 확인하는데에 어려움이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
